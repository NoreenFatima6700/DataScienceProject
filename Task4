import pandas as pd
from sklearn.preprocessing import StandardScaler
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt


# 1: Data Preprocessing

data = pd.read_csv('D:\\Others\\Data_Science\\Python\\HousingData.csv')

# Normalize numerical features
scaler = StandardScaler()
numerical_features = data.select_dtypes(include=['float64', 'int64']).columns
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Preprocess categorical variables (if applicable)
# For example, one-hot encoding for categorical columns:
# data = pd.get_dummies(data, drop_first=True)

# Split features and target
X = data.drop('TAX', axis=1) 
y = data['TAX']

# 2: Model Implementation


# Linear Regression using the Normal Equation
def linear_regression(X, y):
    X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term
    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)  # Normal equation
    return theta

# Train Linear Regression model
theta = linear_regression(X.values, y.values)

# Simplified Random Forest (you can also implement it from scratch with decision trees)
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)
xgb_model.fit(X, y)



# Check for missing values before handling
print("Before handling NaNs:")
print(X.isnull().sum())  # For features
print(y.isnull().sum())  # For target

# Fill missing values
X.fillna(X.mean(), inplace=True)
y.fillna(y.mean(), inplace=True)

# Check again after handling NaNs
print("After handling NaNs:")
print(X.isnull().sum())  # For features
print(y.isnull().sum())  # For target

# Ensure correct feature names during training
rf_model.fit(X, y)
xgb_model.fit(X, y)

# Predictions
y_pred_lr = X.dot(theta[1:]) + theta[0]  # Linear Regression prediction
y_pred_rf = rf_model.predict(X)
y_pred_xgb = xgb_model.predict(X)

# Check for NaNs in predictions
print("Check NaNs in predictions:")
print(np.isnan(y_pred_lr).sum())  # For Linear Regression
print(np.isnan(y_pred_rf).sum())  # For Random Forest
print(np.isnan(y_pred_xgb).sum())  # For XGBoost

# RMSE and R² for each model
def evaluate_model(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return rmse, r2

# Evaluate each model
rmse_lr, r2_lr = evaluate_model(y, y_pred_lr)
rmse_rf, r2_rf = evaluate_model(y, y_pred_rf)
rmse_xgb, r2_xgb = evaluate_model(y, y_pred_xgb)

# Print model evaluation results
print(f"Linear Regression - RMSE: {rmse_lr}, R²: {r2_lr}")
print(f"Random Forest - RMSE: {rmse_rf}, R²: {r2_rf}")
print(f"XGBoost - RMSE: {rmse_xgb}, R²: {r2_xgb}")
