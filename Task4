import pandas as pd
from sklearn.preprocessing import StandardScaler
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# 1: Data Preprocessing

data = pd.read_csv('D:\\Others\\Data_Science\\Python\\HousingData.csv')

# Normalize numerical features
scaler = StandardScaler()
numerical_features = data.select_dtypes(include=['float64', 'int64']).columns
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Preprocess categorical variables (if applicable)
# For example, one-hot encoding for categorical columns:
# data = pd.get_dummies(data, drop_first=True)

# Split features and target
X = data.drop('TAX', axis=1) 
y = data['TAX']

# 2: Model Implementation


# Linear Regression using the Normal Equation
def linear_regression(X, y):
    X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term
    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)  # Normal equation
    return theta

# Train Linear Regression model
theta = linear_regression(X.values, y.values)

# Simplified Random Forest (you can also implement it from scratch with decision trees)
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)
xgb_model.fit(X, y)

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Ensure no missing values in features and target
# Fill missing values in data
data.fillna(data.mean(), inplace=True)

# Split features and target
X = data.drop('Price', axis=1)  # Replace 'Price' with the actual target column
y = data['Price']

# Predictions
y_pred_lr = X.dot(theta[1:]) + theta[0]  # Linear Regression prediction
y_pred_rf = rf_model.predict(X)
y_pred_xgb = xgb_model.predict(X)

# RMSE and R² for each model
def evaluate_model(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return rmse, r2

# Evaluate each model
rmse_lr, r2_lr = evaluate_model(y, y_pred_lr)
rmse_rf, r2_rf = evaluate_model(y, y_pred_rf)
rmse_xgb, r2_xgb = evaluate_model(y, y_pred_xgb)

# Print model evaluation results
print(f"Linear Regression - RMSE: {rmse_lr}, R²: {r2_lr}")
print(f"Random Forest - RMSE: {rmse_rf}, R²: {r2_rf}")
print(f"XGBoost - RMSE: {rmse_xgb}, R²: {r2_xgb}")

# 4: Feature Importance

# Feature Importance for Random Forest
feature_importance_rf = rf_model.feature_importances_
plt.barh(X.columns, feature_importance_rf)
plt.title('Random Forest Feature Importance')
plt.show()

# Feature Importance for XGBoost
feature_importance_xgb = xgb_model.feature_importances_
plt.barh(X.columns, feature_importance_xgb)
plt.title('XGBoost Feature Importance')
plt.show()
